
    




    
\documentclass[11pt]{article}

    
    \usepackage[breakable]{tcolorbox}
    \tcbset{nobeforeafter} % prevents tcolorboxes being placing in paragraphs
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Perception Algorithm - Spam Filter}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \newcommand{\prompt}[4]{
        \llap{{\color{#2}[#3]: #4}}\vspace{-1.25em}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Perception Algorithm: Spam
Filter}\label{perception-algorithm-spam-filter}

Seung Heon (Brian) Shin - Shs522

    Steps:

\subsubsection{1 - Dividing the data set }\label{dividing-the-data-set}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Training Set (set of 4000)
\item
  Validation Set (set of 1000)
\end{enumerate}

\subsubsection{2 - Transforming Data into Feature
Vectors}\label{transforming-data-into-feature-vectors}

\begin{itemize}
\tightlist
\item
  removing words that occur is less than 30 emails
\end{itemize}

\subsubsection{3 - Building the perceptron
algorithm}\label{building-the-perceptron-algorithm}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  perceptron train(data): trains the perceptron classifier using the
  data provided
\item
  perceptron test(w, data): returns test error
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Ignore all words that appear in fewer than X = 30 e-mails
\end{itemize}

\subsubsection{4 - Training the perception classifier, classifying
email, and checking validation
error}\label{training-the-perception-classifier-classifying-email-and-checking-validation-error}

\begin{itemize}
\tightlist
\item
  Training the perceptron liner classifier with training set
\item
  Ensuring training error is 0
\item
  Classifying email within the validation set
\item
  Checking the validation error
\end{itemize}

\subsubsection{5 - Identifying words with highest and lowest
weight}\label{identifying-words-with-highest-and-lowest-weight}

\begin{itemize}
\item
  In order to understand which the workings of the spam classifier, step
  5 attempts to identify:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \item
    15 words with most positive weights
  \item
    15 words with least positive (most negative) weights
  \end{enumerate}
\end{itemize}

\subsubsection{6 - Averaged Perceptron
Algorithm}\label{averaged-perceptron-algorithm}

\begin{itemize}
\tightlist
\item
  Returning the average of all weight vectors considered during
\end{itemize}

\subsubsection{7 - Preventing Overfitting: Limiting maximum number of
passes over
data}\label{preventing-overfitting-limiting-maximum-number-of-passes-over-data}

\begin{itemize}
\tightlist
\item
  Implement a hyperparameter that controls the number of passes over
  data as for large datasets the algorithm may take in numerous
  iterations for minimal change, causing overfitting
\end{itemize}

\subsubsection{8 - Experimenting with varying value of minimum word
appearance
level}\label{experimenting-with-varying-value-of-minimum-word-appearance-level}

\subsubsection{9 - Final Test: Learning with best configuration and
testing on test
dataset}\label{final-test-learning-with-best-configuration-and-testing-on-test-dataset}

    \subsubsection{Part 1}\label{part-1}

This problem set will involve your implementing several variants of the
Perceptron algorithm. Before you can build these models and measure
their performance, split your training data (i.e. spam train.txt) into a
training and validate set, putting the last 1000 emails into the
validation set. Thus, you will have a new training set with 4000 emails
and a validation set with 1000 emails. You will not use spam test.txt
unti

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 1: Defining a function that divides the dataset}

\PY{k}{def} \PY{n+nf}{divideDataSet}\PY{p}{(}\PY{n}{inputFile}\PY{p}{)}\PY{p}{:} 
    \PY{n}{trainingData} \PY{o}{=} \PY{p}{[}\PY{p}{]} 
    \PY{n}{validationData} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    
    \PY{n}{increment} \PY{o}{=} \PY{l+m+mi}{1}
    
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{inputFile}\PY{p}{:}
        \PY{k}{if} \PY{n}{increment} \PY{o}{\PYZgt{}} \PY{l+m+mi}{4000}\PY{p}{:}
            \PY{n}{validationData}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{trainingData}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
        \PY{n}{increment} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        
    \PY{n}{inputFile}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{validationData} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{validationData}\PY{p}{)}
    \PY{n}{trainingData} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{trainingData}\PY{p}{)}
    
    \PY{n}{validationFile} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}validation.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{w}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{trainingFile} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}train.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{w}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

    \PY{n}{trainingFile}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{trainingData}\PY{p}{)} 
    \PY{n}{validationFile}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{validationData}\PY{p}{)}    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Running the divider function}

\PY{n}{dataset} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data/spam\PYZus{}train.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{divideDataSet}\PY{p}{(}\PY{n}{dataset}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \paragraph{Response to Question: Problem with measuring performance
without validation
set}\label{response-to-question-problem-with-measuring-performance-without-validation-set}

Measuring the performance of the final classifier would be problematic
without the validation set because the only way to test the validity and
performance of the model would be to use the test dataset, which is
finite and is reserved strictly for testing. Test dataset should only be
used for testing since exposing our model to the test dataset will
optimize our model based on the real data that should have been used for
the final performance test, creating an unfavorable scenario.

    \subsubsection{Part 3}\label{part-3}

Transform all of the data into feature vectors. Build a vocabulary list
using only the 4000 e-mail training set by finding all words that occur
across the training set. Note that we assume that the data in the
validation and test sets is completely unseen when we train our model,
and thus we do not use any information contained in them. Ignore all
words that appear in fewer than X = 30 e-mails of the 4000 e-mail
training set -- this is both a means of preventing overfitting and of
improving scalability. For each email, transform it into a feature
vector x where the ith entry, xi, is 1 if the ith word in the
vocabulary occurs in the email, and 0 otherwise.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 2: Vectorizing the emails}

\PY{k}{def} \PY{n+nf}{emailVectorizer}\PY{p}{(}\PY{n}{inputData}\PY{p}{)}\PY{p}{:}
    
    \PY{c+c1}{\PYZsh{} Dictionary of vocabularies and their frequency of appearance}
    \PY{n}{vocabFrequencyDict} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}} 
    
    \PY{c+c1}{\PYZsh{} List composed of the emails vectorized in terms of their constituent words}
    \PY{n}{vectorList} \PY{o}{=} \PY{p}{[}\PY{p}{]} 

    \PY{c+c1}{\PYZsh{} List identifying whether email is spam or ham}
    \PY{n}{spamIdentify} \PY{o}{=} \PY{p}{[}\PY{p}{]} 
    
    \PY{n}{dataList} \PY{o}{=} \PY{n}{inputData}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dataList}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{vocabOccurence} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        \PY{n}{vocabulary} \PY{o}{=} \PY{n}{dataList}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{vocab} \PY{o+ow}{in} \PY{n}{vocabulary} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{:}
            \PY{k}{if} \PY{n}{vocab} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{vocabOccurence}\PY{p}{:}
                \PY{k}{if} \PY{n}{vocab} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{vocabFrequencyDict}\PY{p}{:}
                    \PY{n}{vocabFrequencyDict}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{vocab}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{vocabFrequencyDict}\PY{p}{[}\PY{n}{vocab}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                \PY{n}{vocabOccurence}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{vocab}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}\PY{p}{)}
    
        \PY{c+c1}{\PYZsh{} Since the first index of vocabulary is 0/1, identifying whether the email is spam or ham}
        \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{vocabulary}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{vectorList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{vocabOccurence}\PY{p}{)} 
            \PY{n}{spamIdentify}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{int}\PY{p}{(}\PY{n}{vocabulary}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
            
    \PY{k}{return} \PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{vectorList} \PY{p}{,} \PY{n}{spamIdentify}
\end{Verbatim}
\end{tcolorbox}

    \paragraph{emailVectorizer(inputData) is a function that vectorizes the
email by
creating:}\label{emailvectorizerinputdata-is-a-function-that-vectorizes-the-email-by-creating}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  vocabFrequencyDict - a dictionary that contains vocabulary and its
  frequency of appearance in emails
\item
  vectorList - a list of dictionaries that contain the vocabulary within
  individual email
\item
  spamIdentify - a list that identifies whether email is spam or ham
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 2: Building the vocabulary list }

\PY{k}{def} \PY{n+nf}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{minEmail}\PY{p}{)}\PY{p}{:}
    \PY{n}{wordMap} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}} 
    \PY{n}{wordList} \PY{o}{=} \PY{p}{[}\PY{p}{]} 
    \PY{n}{inc} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{vocabFrequencyDict}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:} 
        \PY{k}{if} \PY{n}{vocabFrequencyDict}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{minEmail}\PY{p}{:}
            \PY{n}{wordList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{key}\PY{p}{)} 
            \PY{n}{wordMap}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{key}\PY{p}{:} \PY{n}{inc}\PY{p}{\PYZcb{}}\PY{p}{)}
            \PY{n}{inc} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{wordMap} \PY{p}{,} \PY{n}{wordList}
\end{Verbatim}
\end{tcolorbox}

    \paragraph{makeWordList((vocabFrequencyDict , minEmail) creates a
vocabulary list of words that appear in more than 'minemail'
emails}\label{makewordlistvocabfrequencydict-minemail-creates-a-vocabulary-list-of-words-that-appear-in-more-than-minemail-emails}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  wordMap - a dictionary that maps the vocabulary to index
\item
  wordList - a list of vocabulary that appears in more than 'minEmail'
  emails
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 2: Building the vocabulary list }

\PY{k}{def} \PY{n+nf}{makeFeatureList}\PY{p}{(}\PY{n}{wordMap}\PY{p}{,} \PY{n}{emailVector}\PY{p}{)}\PY{p}{:}
    \PY{n}{featureList} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{emailDict} \PY{o+ow}{in} \PY{n}{emailVector}\PY{p}{:}
        \PY{n}{features} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wordMap}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{]}
        \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{emailDict}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{n}{wordMap}\PY{p}{:}
                \PY{n}{features}\PY{p}{[}\PY{n}{wordMap}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{featureList}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{features}\PY{p}{)}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{featureList}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \paragraph{makeFeatureList(wordMap, emailVector) creates a feature list
that indicates whether a specific vocabulary exists in the
wordMap.}\label{makefeaturelistwordmap-emailvector-creates-a-feature-list-that-indicates-whether-a-specific-vocabulary-exists-in-the-wordmap.}

\begin{itemize}
\tightlist
\item
  Numpy is used in order to increase performance (i.e. dot products)
\end{itemize}

Source:
https://becominghuman.ai/an-essential-guide-to-numpy-for-machine-learning-in-python-5615e1758301

    \subsubsection{Part 3}\label{part-3}

Implement the functions perceptron train(data) and perceptron test(w,
data). The function perceptron train(data) trains a perceptron
classifier using the examples provided to the function, and should
return w, k, and iter, the final classification vector, the number of
updates (mistakes) performed, and the number of passes through the data,
respectively. You may assume that the input data provided to your
function is linearly separable (so the stopping criterion should be that
all points are correctly classified). For the corner case of w Â· x =
0, predict the +1 (spam) class.

The function perceptron test(w, data) should take as input the weight
vector w (the classification vector to be used) and a set of examples.
The function should return the test error, i.e. the fraction of examples
that are misclassified by w.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{122}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{trainPerceptron}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}\PY{p}{:}
    \PY{n}{a} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{spamList} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}
    \PY{n}{featureList} \PY{o}{=} \PY{n}{makeFeatureList}\PY{p}{(}\PY{n}{wordMap}\PY{p}{,} \PY{n}{emailVector}\PY{p}{)}

    \PY{n}{weight} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{weight}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

    
    \PY{n}{totalError} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} records the total number of updates}
    \PY{n}{curIter} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} Set to \PYZhy{}1 to enter loop, indicates the number of updates in the current epoch}
    \PY{n}{repeatCount} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} records the number of iterations ( epochs)}

    \PY{k}{while} \PY{o+ow}{not} \PY{p}{(}\PY{n}{curIter} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n}{curIter} \PY{o}{=} \PY{l+m+mi}{0} 
        \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{while} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Get the checker we use to multiple with dot product}
            \PY{k}{if} \PY{n}{spamList}\PY{p}{[}\PY{n}{count}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{checker} \PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            \PY{k}{else}\PY{p}{:} 
                \PY{n}{checker}\PY{o}{=}\PY{l+m+mi}{1}
            \PY{c+c1}{\PYZsh{} Calculate dot}
            \PY{n}{product} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{,} \PY{n}{weight}\PY{p}{)} 
            \PY{k}{if} \PY{n}{product} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{product} \PY{o}{=} \PY{l+m+mi}{1}

            \PY{k}{if} \PY{n}{product} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{product} \PY{o}{=} \PY{l+m+mi}{1}
                
            \PY{k}{if} \PY{n}{checker}\PY{o}{*}\PY{n}{product} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{curIter} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1} 
                \PY{n}{weight} \PY{o}{=} \PY{n}{weight} \PY{o}{+} \PY{n}{checker}\PY{o}{*}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{)}
            \PY{n}{count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{totalError} \PY{o}{+}\PY{o}{=} \PY{n}{curIter}
        \PY{n}{repeatCount} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    
    \PY{k}{return} \PY{n}{weight} \PY{p}{,} \PY{n}{totalError} \PY{p}{,} \PY{n}{repeatCount}

\PY{k}{def} \PY{n+nf}{testPerceptron}\PY{p}{(}\PY{n}{weight} \PY{p}{,} \PY{n}{dataInput}\PY{p}{)}\PY{p}{:} 
    \PY{c+c1}{\PYZsh{} Getting the featureList}
    \PY{n}{a} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{spamList} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}
    \PY{n}{featureList} \PY{o}{=} \PY{n}{makeFeatureList}\PY{p}{(}\PY{n}{wordMap} \PY{p}{,} \PY{n}{emailVector}\PY{p}{)}
    \PY{n}{noErrors} \PY{o}{=} \PY{l+m+mi}{0} 
    \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{while} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Get the checker we use to multiple with dot product}
        \PY{k}{if} \PY{n}{spamList}\PY{p}{[}\PY{n}{count}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:} 
            \PY{n}{checker} \PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
        \PY{k}{else}\PY{p}{:} 
            \PY{n}{checker}\PY{o}{=}\PY{l+m+mi}{1}
            
        \PY{c+c1}{\PYZsh{} Calculate dot}
        \PY{n}{product} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{,} \PY{n}{weight}\PY{p}{)} 
        \PY{k}{if} \PY{n}{product} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{product} \PY{o}{=} \PY{l+m+mi}{1}

        \PY{c+c1}{\PYZsh{} Check sign agreement}
        \PY{k}{if} \PY{n}{checker}\PY{o}{*}\PY{n}{product} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{noErrors} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        
        \PY{n}{count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n+nb}{float}\PY{p}{(}\PY{n}{noErrors}\PY{o}{/}\PY{n}{count}\PY{p}{)}
                
                
                
                
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{123}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{readData}\PY{p}{(}\PY{n}{trainInput}\PY{p}{,} \PY{n}{validationInput}\PY{p}{)}\PY{p}{:}
    \PY{n}{trainData} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{trainInput}\PY{p}{)}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}
    \PY{n}{validationData} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{validationInput}\PY{p}{)}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)} 
    
    \PY{k}{return} \PY{n}{trainData}\PY{p}{,} \PY{n}{validationData}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Part 4}\label{part-4}

Train the linear classifier using your training set. How many mistakes
are made before the algorithm terminates? Test your implementation of
perceptron test by running it with the learned parameters and the
training data, making sure that the training error is zero. Next,
classify the emails in your validation set. What is the validation
error?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{124}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 4}

\PY{n}{trainData}\PY{p}{,} \PY{n}{validationData} \PY{o}{=} \PY{n}{readData}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}train.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}validation.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{a} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{trainData}\PY{p}{)}
\PY{n}{wordMap}\PY{p}{,} \PY{n}{wordList} \PY{o}{=} \PY{n}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}
    
\PY{n}{weight}\PY{p}{,} \PY{n}{totalError}\PY{p}{,} \PY{n}{repeatCount} \PY{o}{=} \PY{n}{trainPerceptron}\PY{p}{(}\PY{n}{trainData}\PY{p}{)}
\PY{n}{testError} \PY{o}{=} \PY{n}{testPerceptron}\PY{p}{(}\PY{n}{weight} \PY{p}{,} \PY{n}{trainData}\PY{p}{)} 
\PY{n}{testErrorValidate} \PY{o}{=} \PY{n}{testPerceptron}\PY{p}{(}\PY{n}{weight} \PY{p}{,} \PY{n}{validationData}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Set Error Level: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{testError}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of mistakes made by trainPerceptron:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{totalError}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation Set Error Level: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{testErrorValidate}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
This is the error on the training set:  0.0
Number of mistakes made by trainPerceptron: 447
This is the error on the validation set:  0.02
\end{Verbatim}

    \subsubsection{Part 5}\label{part-5}

To better understand how the spam classifier works, we can inspect the
parameters to see which words the classifier thinks are the most
predictive of spam. Using the vocabulary list together with the
parameters learned in the previous question, output the 15 words with
the most positive weights. What are they? Which 15 words have the most
negative weights?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{135}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{spamHamIdentify}\PY{p}{(}\PY{n}{weight}\PY{p}{,} \PY{n}{limit}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Mapping weight to vocab}
    \PY{n}{sortVocab} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{vocab}\PY{p}{:}\PY{n}{weight} \PY{k}{for} \PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{n}{weight}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{wordList}\PY{p}{,} \PY{n}{weight}\PY{p}{)}\PY{p}{\PYZcb{}}
    
    \PY{c+c1}{\PYZsh{} Sorting dictionary using sorted()}
    \PY{n}{sortVocab}\PY{o}{=}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{sortVocab}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    
    \PY{n}{spamList}\PY{o}{=}\PY{n}{sortVocab}\PY{p}{[}\PY{p}{:}\PY{n}{limit}\PY{p}{]} 
    \PY{n}{hamList}\PY{o}{=}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{reversed}\PY{p}{(}\PY{n}{sortVocab}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{limit}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
    
    \PY{k}{return} \PY{p}{(}\PY{n}{spamList}\PY{p}{,}\PY{n}{hamList}\PY{p}{)}    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{136}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{trainData}\PY{p}{,} \PY{n}{validationData} \PY{o}{=} \PY{n}{readData}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}train.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}validation.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{a} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{trainData}\PY{p}{)}
\PY{n}{wordMap}\PY{p}{,} \PY{n}{wordList} \PY{o}{=} \PY{n}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}

\PY{n}{weight}\PY{p}{,} \PY{n}{totalError}\PY{p}{,} \PY{n}{repeatCount} \PY{o}{=} \PY{n}{trainPerceptron}\PY{p}{(}\PY{n}{trainData}\PY{p}{)}

\PY{n}{spamList}\PY{p}{,}\PY{n}{hamList}\PY{o}{=}\PY{n}{spamHamIdentify}\PY{p}{(}\PY{n}{weight}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{15 most positive vocabulary and weight in order = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{hamList}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{15 most negative vocabulary and weight in order= }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{spamList}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
15 most positive vocabulary and weight in order =  [('sight', 19), ('our', 17),
('yourself', 16), ('remov', 16), ('guarante', 15), ('market', 15), ('nbsp', 15),
('pleas', 15), ('click', 15), ('these', 15),
('deathtospamdeathtospamdeathtospam', 14), ('ever', 14), ('present', 14),
('your', 14), ('major', 13)]
15 most negative vocabulary and weight in order=  [('but', -15), ('wrote', -15),
('prefer', -15), ('and', -14), ('i', -13), ('reserv', -13), ('on', -12),
('still', -12), ('technolog', -12), ('sinc', -11), ('copyright', -11), ('url',
-11), ('instead', -11), ('upgrad', -11), ('recipi', -11)]
\end{Verbatim}

    \subsubsection{Part 6}\label{part-6}

Implement the averaged perceptron algorithm, which is the same as your
current implementation but which, rather than returning the final weight
vector, returns the average of all weight vectors considered during the
algorithm (including examples where no mistake was made). Averaging
reduces the variance between the different vectors,

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{105}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{trainPerceptronAvg}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}\PY{p}{:}
\PY{c+c1}{\PYZsh{} Getting the featureList}
    \PY{n}{a} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{spamList} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}
    \PY{n}{featureList} \PY{o}{=} \PY{n}{makeFeatureList}\PY{p}{(}\PY{n}{wordMap}\PY{p}{,} \PY{n}{emailVector}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Actual Perceptron algorithm in play}
    \PY{n}{weight} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{weight}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{n}{weightAvg} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        
        
    \PY{n}{curIter} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} Set to \PYZhy{}1 to enter loop, indicates the number of updates in the current epoch }
    \PY{n}{repeatCount} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} records the number of iterations (epochs)}
    \PY{k}{while} \PY{o+ow}{not} \PY{p}{(}\PY{n}{curIter} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:} 
        \PY{n}{curIter} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{while} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Get the y we use to multiple with dot product}
            \PY{k}{if} \PY{n}{spamList}\PY{p}{[}\PY{n}{count}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{checker} \PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} 
            \PY{k}{else}\PY{p}{:}
                \PY{n}{checker}\PY{o}{=}\PY{l+m+mi}{1}
                
        \PY{c+c1}{\PYZsh{} Calculate dot}
            \PY{n}{product} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{,} \PY{n}{weight}\PY{p}{)}
            \PY{k}{if} \PY{n}{product} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{product} \PY{o}{=} \PY{l+m+mi}{1}
            
        \PY{c+c1}{\PYZsh{} Check sign agreement}
            \PY{k}{if} \PY{n}{checker}\PY{o}{*}\PY{n}{product} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{curIter} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1} 
                \PY{n}{weight} \PY{o}{+}\PY{o}{=} \PY{n}{checker}\PY{o}{*}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{)}
        

            \PY{n}{count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{n}{noPasses} \PY{o}{=} \PY{p}{(}\PY{n}{repeatCount}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{count}
            \PY{n}{weightAvg} \PY{o}{=} \PY{p}{(}\PY{n}{weightAvg}\PY{o}{*}\PY{p}{(}\PY{n}{noPasses} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{weight}\PY{p}{)}\PY{o}{/} \PY{n}{noPasses}

        \PY{n}{repeatCount} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{weightAvg}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Part 7}\label{part-7}

Add an argument to both the perceptron and the averaged perceptron that
controls the maximum number of passes over the data. This is an
important hyperparameter because for large training sets, the perceptron
algorithm can take many iterations just changing a small subset of the
point -\/- leading to overfitting.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{106}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 7}

\PY{k}{def} \PY{n+nf}{trainPerceptron}\PY{p}{(}\PY{n}{dataInput}\PY{p}{,} \PY{n}{noPass}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Getting the featureList}
    \PY{k}{global} \PY{n}{wordMap} 
    \PY{k}{global} \PY{n}{wordList} 
    \PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{spamList} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}
    \PY{n}{wordMap}\PY{p}{,} \PY{n}{wordList} \PY{o}{=} \PY{n}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}
    \PY{n}{featureList} \PY{o}{=} \PY{n}{makeFeatureList}\PY{p}{(}\PY{n}{wordMap} \PY{p}{,} \PY{n}{emailVector}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Actual Perceptron algorithm in play}
    \PY{n}{weight} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{weight}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} bias = 1 bias not required}
    \PY{n}{totalError} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} records the total number of updates}
    \PY{n}{curIter} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} Set to \PYZhy{}1 to enter loop }
    \PY{n}{repeatCount} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} records the number of iterations}
    
    \PY{k}{while} \PY{o+ow}{not} \PY{p}{(}\PY{n}{curIter} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{p}{(}\PY{n}{noPass} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o+ow}{or} \PY{n}{repeatCount} \PY{o}{\PYZlt{}} \PY{n}{noPass}\PY{p}{)}\PY{p}{:}
        \PY{n}{curIter} \PY{o}{=} \PY{l+m+mi}{0} 
        \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}

        \PY{k}{while} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{spamList}\PY{p}{[}\PY{n}{count}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:} 
                \PY{n}{checker} \PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            \PY{k}{else}\PY{p}{:} 
                \PY{n}{checker}\PY{o}{=}\PY{l+m+mi}{1}
                
            \PY{c+c1}{\PYZsh{} Calculate dot}
            \PY{n}{product} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{,} \PY{n}{weight}\PY{p}{)} 
            \PY{k}{if} \PY{n}{product} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{product} \PY{o}{=} \PY{l+m+mi}{1}
                
            \PY{c+c1}{\PYZsh{} Check sign agreement}
            \PY{k}{if} \PY{n}{checker}\PY{o}{*}\PY{n}{product} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{curIter} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1} 
                \PY{n}{weight} \PY{o}{=} \PY{n}{weight}\PY{o}{+} \PY{n}{checker}\PY{o}{*}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{)}
            
            \PY{n}{count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{totalError} \PY{o}{+}\PY{o}{=} \PY{n}{curIter}
        \PY{n}{repeatCount} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        
    \PY{k}{return} \PY{n}{weight} \PY{p}{,} \PY{n}{totalError} \PY{p}{,} \PY{n}{repeatCount}

\PY{k}{def} \PY{n+nf}{trainPerceptronAvg}\PY{p}{(}\PY{n}{dataInput}\PY{p}{,} \PY{n}{noPass}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} Getting the featureList}

    \PY{c+c1}{\PYZsh{} Global variables for outside function modification}
    \PY{k}{global} \PY{n}{wordMap} 
    \PY{k}{global} \PY{n}{wordList}
    
    \PY{n}{wordMap} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{spamList} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{dataInput}\PY{p}{)}
    \PY{n}{wordMap}\PY{p}{,} \PY{n}{wordList} \PY{o}{=} \PY{n}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}
    \PY{n}{featureList} \PY{o}{=} \PY{n}{makeFeatureList}\PY{p}{(}\PY{n}{wordMap} \PY{p}{,} \PY{n}{emailVector}\PY{p}{)}
    
    \PY{n}{weight} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{weight}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} numpy array for operational modifications}
    \PY{n}{weightAvg} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    
    \PY{n}{curIter} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} 
    \PY{n}{repeatCount} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{while} \PY{o+ow}{not} \PY{p}{(}\PY{n}{curIter} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{p}{(}\PY{n}{noPass} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o+ow}{or} \PY{n}{repeatCount} \PY{o}{\PYZlt{}} \PY{n}{noPass}\PY{p}{)}\PY{p}{:} 
        \PY{n}{curIter} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{while} \PY{n}{count} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{featureList}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{spamList}\PY{p}{[}\PY{n}{count}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{checker} \PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1} 
            \PY{k}{else}\PY{p}{:}
                \PY{n}{checker}\PY{o}{=}\PY{l+m+mi}{1}
                
            \PY{c+c1}{\PYZsh{} Calculate dot}
            \PY{n}{product} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{,} \PY{n}{weight}\PY{p}{)}
            \PY{k}{if} \PY{n}{product} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:} 
                \PY{n}{product} \PY{o}{=} \PY{l+m+mi}{1}

            \PY{c+c1}{\PYZsh{} Check sign agreement}
            \PY{k}{if} \PY{n}{checker}\PY{o}{*}\PY{n}{product} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{curIter} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                \PY{n}{weight} \PY{o}{+}\PY{o}{=} \PY{n}{checker}\PY{o}{*}\PY{p}{(}\PY{n}{featureList}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{)}
            
            \PY{n}{count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{n}{noPasses} \PY{o}{=} \PY{p}{(}\PY{n}{repeatCount}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{count}
            
            \PY{n}{weightAvg} \PY{o}{=} \PY{p}{(}\PY{n}{weightAvg}\PY{o}{*}\PY{p}{(}\PY{n}{noPasses} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{weight}\PY{p}{)} \PY{o}{/} \PY{n}{noPasses}
        \PY{n}{repeatCount} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
    \PY{k}{return} \PY{n}{weightAvg}
    
\end{Verbatim}
\end{tcolorbox}

    The error for the training set convergedto 0.018 after 10 iterations.
Hence hyperparamter was set accordingly.

    \subsubsection{Part 8}\label{part-8}

Experiment with various maximum iterations on the two algorithms
checking performance on the validation set. Optionally you can try to
change X from question 2. Report the best validation error for the two
algorithms.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{113}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 8}

\PY{n}{trainData}\PY{p}{,} \PY{n}{validationData} \PY{o}{=} \PY{n}{readData}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}train.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}validation.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{a} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{trainData}\PY{p}{)}
\PY{n}{wordMap}\PY{p}{,} \PY{n}{wordList} \PY{o}{=} \PY{n}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{weight}\PY{p}{,} \PY{n}{totalError}\PY{p}{,} \PY{n}{repeatCount} \PY{o}{=} \PY{n}{trainPerceptron}\PY{p}{(}\PY{n}{trainData} \PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} max 11}
    \PY{n}{testErrorValidate} \PY{o}{=} \PY{n}{testPerceptron}\PY{p}{(}\PY{n}{weight} \PY{p}{,} \PY{n}{validationData}\PY{p}{)}
    \PY{n}{weightAvg} \PY{o}{=} \PY{n}{trainPerceptronAvg}\PY{p}{(}\PY{n}{trainData} \PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{testErrorValidateAverage} \PY{o}{=} \PY{n}{testPerceptron}\PY{p}{(}\PY{n}{weightAvg} \PY{p}{,} \PY{n}{validationData}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{iteration \PYZhy{} trainPerceptron:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{testErrorValidate}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{iteration \PYZhy{} trainPerceptronAvg:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{testErrorValidateAverage}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\# 1 iteration - trainPerceptron: 0.028
\# 1 iteration - trainPerceptronAvg: 0.02
\# 2 iteration - trainPerceptron: 0.029
\# 2 iteration - trainPerceptronAvg: 0.02
\# 3 iteration - trainPerceptron: 0.047
\# 3 iteration - trainPerceptronAvg: 0.018
\# 4 iteration - trainPerceptron: 0.018
\# 4 iteration - trainPerceptronAvg: 0.019
\# 5 iteration - trainPerceptron: 0.027
\# 5 iteration - trainPerceptronAvg: 0.015
\# 6 iteration - trainPerceptron: 0.024
\# 6 iteration - trainPerceptronAvg: 0.02
\# 7 iteration - trainPerceptron: 0.02
\# 7 iteration - trainPerceptronAvg: 0.021
\# 8 iteration - trainPerceptron: 0.025
\# 8 iteration - trainPerceptronAvg: 0.02
\# 9 iteration - trainPerceptron: 0.02
\# 9 iteration - trainPerceptronAvg: 0.019
\# 10 iteration - trainPerceptron: 0.02
\# 10 iteration - trainPerceptronAvg: 0.018
\end{Verbatim}

    \subsubsection{Part 9}\label{part-9}

Combine the training set and the validation set (i.e. us all of
spam\_train.txt) and learn using the best of the configurations
previously found. You do not need to rebuild the vocabulary when
re-training on the train + validate set. What is the error on the test
set (i.e. spam\_test.txt).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{114}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Part 9}

\PY{n}{trainData}\PY{p}{,} \PY{n}{validationData} \PY{o}{=} \PY{n}{readData}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}train.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spam\PYZus{}validation.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{testData} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data/spam\PYZus{}test.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}

\PY{n}{vocabFrequencyDict} \PY{p}{,} \PY{n}{emailVector} \PY{p}{,} \PY{n}{a} \PY{o}{=} \PY{n}{emailVectorizer}\PY{p}{(}\PY{n}{trainData}\PY{p}{)}
\PY{n}{wordMap}\PY{p}{,} \PY{n}{wordList} \PY{o}{=} \PY{n}{makeWordList}\PY{p}{(}\PY{n}{vocabFrequencyDict}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}

\PY{n}{finalWeight} \PY{o}{=} \PY{n}{trainPerceptronAvg}\PY{p}{(}\PY{n}{validationData}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)} 
\PY{n}{finalError} \PY{o}{=} \PY{n}{testPerceptron}\PY{p}{(}\PY{n}{finalWeight} \PY{p}{,} \PY{n}{testData}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error on Test Set:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{finalError}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Error on Test Set: 0.042
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
