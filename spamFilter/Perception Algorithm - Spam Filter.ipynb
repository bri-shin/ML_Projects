{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perception Algorithm: Spam Filter\n",
    "Seung Heon (Brian) Shin - Shs522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "### 1 - <b>Dividing the data set </b>\n",
    "\n",
    "\n",
    "a) Training Set (set of 4000)\n",
    "\n",
    "b) Validation Set (set of 1000)\n",
    "\n",
    "### 2 - <b>Transforming Data into Feature Vectors</b>\n",
    "\n",
    "- removing words that occur is less than 30 emails\n",
    "\n",
    "### 3 - <b>Building the perceptron algorithm</b>\n",
    "\n",
    "a) perceptron train(data): trains the perceptron classifier using the data provided\n",
    "\n",
    "b) perceptron test(w, data): returns test error\n",
    "\n",
    "- Ignore all words that appear in fewer than X = 30 e-mails\n",
    "\n",
    "### 4 - Training the perception classifier, classifying email, and checking validation error\n",
    "\n",
    "- Training the perceptron liner classifier with training set\n",
    "- Ensuring training error is 0\n",
    "- Classifying email within the validation set\n",
    "- Checking the validation error\n",
    "\n",
    "### 5 - Identifying words with highest and lowest weight\n",
    "\n",
    "- In order to understand which the workings of the spam classifier, step 5 attempts to identify:\n",
    "\n",
    "    a) 15 words with most positive weights \n",
    "    \n",
    "    b) 15 words with least positive (most negative) weights\n",
    "    \n",
    "### 6 - Averaged Perceptron Algorithm \n",
    "\n",
    "- Returning the average of all weight vectors considered during \n",
    "\n",
    "### 7 - Preventing Overfitting: Limiting maximum number of passes over data\n",
    "\n",
    "- Implement a hyperparameter that controls the number of passes over data as for large datasets the algorithm may take in numerous iterations for minimal change, causing overfitting\n",
    "\n",
    "### 8 - Experimenting with varying value of minimum word appearance level\n",
    "\n",
    "### 9 - Final Test: Learning with best configuration and testing on test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "This problem set will involve your implementing several variants of the Perceptron algorithm. Before you can build these models and measure their performance, split your\n",
    "training data (i.e. spam train.txt) into a training and validate set, putting the last 1000\n",
    "emails into the validation set. Thus, you will have a new training set with 4000 emails and\n",
    "a validation set with 1000 emails. You will not use spam test.txt unti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Defining a function that divides the dataset\n",
    "\n",
    "def divideDataSet(inputFile): \n",
    "    \n",
    "    # Inital arrays for training dataset and validation dataset defined\n",
    "    trainingData = [] \n",
    "    validationData = []\n",
    "    \n",
    "    increment = 1\n",
    "    \n",
    "    # Looping through the input (which is 5000 lines) to divide the dataset into 4000 training dataset and 1000 validation dataset\n",
    "    for i in inputFile:\n",
    "        if increment > 4000:\n",
    "            validationData.append(i)\n",
    "        else:\n",
    "            trainingData.append(i)\n",
    "        increment += 1\n",
    "        \n",
    "    inputFile.close()\n",
    "    \n",
    "    # Modidying the data type from an array to a string for preparation\n",
    "    validationData = \" \".join(validationData)\n",
    "    trainingData = \" \".join(trainingData)\n",
    "    \n",
    "    # Creating separate text files for validation dataset and training dataset\n",
    "    validationFile = open(\"spam_validation.txt\", \"w\")\n",
    "    trainingFile = open(\"spam_train.txt\", \"w\")\n",
    "\n",
    "    # Writing\n",
    "    trainingFile.write(trainingData) \n",
    "    validationFile.write(validationData)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the divider function\n",
    "\n",
    "dataset = open(\"Data/spam_train.txt\", \"r\")\n",
    "divideDataSet(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response to Question: Problem with measuring performance without validation set\n",
    "Measuring the performance of the final classifier would be problematic without the validation set because the only way to test the validity and performance of the model would be to use the test dataset, which is finite and is reserved strictly for testing. Test dataset should only be used for <b>testing</b> since exposing our model to the test dataset will optimize our model based on the real data that should have been used for the final performance test, creating an unfavorable scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Transform all of the data into feature vectors. Build a vocabulary list using only the 4000 e-mail training set by finding all words that occur across the training set. Note that we assume that the data in the validation and test sets is completely unseen when we train our model, and thus we do not use any information contained in them. Ignore all words that appear in fewer than X = 30 e-mails of the 4000 e-mail training set – this is both a means of preventing overfitting and of improving scalability. For each email, transform it into a feature vector \u001d",
    "x where the ith entry, xi, is 1 if the ith word in the vocabulary occurs in the email, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Vectorizing the emails\n",
    "\n",
    "def emailVectorizer(inputData):\n",
    "    \n",
    "    # Dictionary of vocabularies and their frequency of appearance\n",
    "    vocabFrequencyDict = {} \n",
    "    \n",
    "    # Array composed of the emails vectorized in terms of their constituent words\n",
    "    vectorList = [] \n",
    "\n",
    "    # List identifying whether email is spam or ham\n",
    "    spamIdentify = [] \n",
    "    \n",
    "    # inputData is converted into an array that is split by newline\n",
    "    dataList = inputData.split(\"\\n\")\n",
    "    \n",
    "    #looping through the number of dataset inside the dataList array (4000)\n",
    "    for i in range(len(dataList)):\n",
    "        # Dictionary for vocabulary and its frequency of appearance within email 'i'\n",
    "        vocabOccurence = {}\n",
    "        \n",
    "        # An array of inidividual words within email i\n",
    "        vocabulary = dataList[i].split()\n",
    "        \n",
    "        # *** The for loop starts form index 1 as index 0 is a number 0/1 that identifies spam or ham\n",
    "        for vocab in vocabulary [1:]:\n",
    "            if vocab not in vocabOccurence:\n",
    "                if vocab not in vocabFrequencyDict:\n",
    "                    vocabFrequencyDict.update({vocab: 1})\n",
    "                else:\n",
    "                    vocabFrequencyDict[vocab] += 1\n",
    "                vocabOccurence.update({vocab: 1})\n",
    "    \n",
    "        # As aforementioned, since the first index of vocabulary is 0/1, identifying whether the email is spam or ham\n",
    "        if len(vocabulary) > 0:\n",
    "            vectorList.append(vocabOccurence) \n",
    "            spamIdentify.append(int(vocabulary[0]))\n",
    "            \n",
    "    return vocabFrequencyDict , vectorList , spamIdentify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### emailVectorizer(inputData) is a function that vectorizes the email by creating:\n",
    "1. vocabFrequencyDict - a dictionary that contains vocabulary and its frequency of appearance in emails\n",
    "2. vectorList - a list of dictionaries that contain the vocabulary within individual email\n",
    "3. spamIdentify - a list that identifies whether email is spam or ham \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Building the vocabulary list \n",
    "\n",
    "def makeWordList(vocabFrequencyDict , minEmail):\n",
    "    # An Array of words in emails and Dictionary of words mapped with numerical index\n",
    "    wordList = [] \n",
    "    wordMap = {} \n",
    "    \n",
    "    inc = 0\n",
    "    \n",
    "    # Looping through the Frequency Dictionary in order to filter out words appearing in less than 'minemail' emails\n",
    "    for key in vocabFrequencyDict.keys(): \n",
    "        # minEmail value can be changed by the user as needs be\n",
    "        if vocabFrequencyDict[key] >= minEmail:\n",
    "            wordList.append(key) \n",
    "            # Creaitng this dictionary of words allow us to easily identify and access \n",
    "            wordMap.update({key: inc})\n",
    "            inc += 1\n",
    "            \n",
    "    return wordMap , wordList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### makeWordList((vocabFrequencyDict , minEmail) creates a vocabulary list of words that appear in more than 'minemail' emails\n",
    "\n",
    "1. wordMap - a dictionary that maps the vocabulary to index\n",
    "2. wordList - a list of vocabulary that appears in more than 'minEmail' emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Building the feature list using the vocab list\n",
    "\n",
    "def makeFeatureList(wordMap, emailVector):\n",
    "    # 2D Array that contains features\n",
    "    featureList = []\n",
    "    for emailDict in emailVector:\n",
    "        features = [0 for i in range(len(wordMap.keys()))]\n",
    "        for key in emailDict.keys():\n",
    "            if key in wordMap:\n",
    "                features[wordMap[key]] = 1\n",
    "        featureList.append(features)\n",
    "    return np.array(featureList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### makeFeatureList(wordMap, emailVector) creates a feature list that indicates whether a specific vocabulary exists in the wordMap. \n",
    "- Numpy is used in order to increase performance (i.e. dot products) \n",
    "\n",
    "Source: https://becominghuman.ai/an-essential-guide-to-numpy-for-machine-learning-in-python-5615e1758301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Implement the functions perceptron train(data) and perceptron test(w, data).\n",
    "The function perceptron train(data) trains a perceptron classifier using the examples\n",
    "provided to the function, and should return \u001d",
    "w, k, and iter, the final classification vector,\n",
    "the number of updates (mistakes) performed, and the number of passes through the data,\n",
    "respectively. You may assume that the input data provided to your function is linearly\n",
    "separable (so the stopping criterion should be that all points are correctly classified). For\n",
    "the corner case of \u001d",
    "w · \u001d",
    "x = 0, predict the +1 (spam) class.\n",
    "\n",
    "The function perceptron test(w, data) should take as input the weight vector \u001d",
    "w (the\n",
    "classification vector to be used) and a set of examples. The function should return the test\n",
    "error, i.e. the fraction of examples that are misclassified by \u001d",
    "w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainPerceptron(dataInput):\n",
    "    # Retrieving vectorized emails and the feature list\n",
    "    a , emailVector , spamList = emailVectorizer(dataInput)\n",
    "    featureList = makeFeatureList(wordMap, emailVector)\n",
    "\n",
    "    weight = []\n",
    "    for i in range(len((featureList[0]))):\n",
    "        weight.append(0)\n",
    "\n",
    "    # Epochs: the number of repeated iterations    \n",
    "    repeatCount = 0 \n",
    "\n",
    "    # Total Error on updates\n",
    "    totalError = 0 \n",
    "\n",
    "    # Current iteration's update num\n",
    "    curIter = -1 \n",
    "\n",
    "\n",
    "    while not (curIter == 0):\n",
    "        curIter = 0 \n",
    "        count = 0\n",
    "        while count < len(featureList):\n",
    "            if spamList[count] == 0:\n",
    "                checker =-1\n",
    "            else: \n",
    "                checker=1\n",
    "            # Dot product is calculated by multiplying feature and weight\n",
    "            product = np.dot(featureList[count], weight) \n",
    "            if product == 0:\n",
    "                product = 1\n",
    "            # Use checker to identify sign\n",
    "            if product*checker <= 0:\n",
    "                curIter += 1 \n",
    "                weight += checker*(featureList[count])\n",
    "            count += 1\n",
    "        # Total Error incremented with current iteration's update number\n",
    "        totalError += curIter\n",
    "        repeatCount += 1\n",
    "    \n",
    "    return weight , totalError , repeatCount\n",
    "\n",
    "def testPerceptron(weight , dataInput): \n",
    "    # Getting the featureList\n",
    "    a , emailVector , spamList = emailVectorizer(dataInput)\n",
    "    featureList = makeFeatureList(wordMap , emailVector)\n",
    "    noErrors = 0 \n",
    "    count = 0\n",
    "    while count < len(featureList):\n",
    "    # Get the checker we use to multiple with dot product\n",
    "        if spamList[count] == 0: \n",
    "            checker =-1\n",
    "        else: \n",
    "            checker=1\n",
    "        # Dot product is calculated by multiplying feature and weight\n",
    "        product = np.dot(featureList[count], weight) \n",
    "        if product == 0:\n",
    "            product = 1\n",
    "        # Use checker to identify sign\n",
    "        if product*checker <= 0:\n",
    "            noErrors += 1\n",
    "        \n",
    "        count += 1\n",
    "    return float(noErrors/count)\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(trainInput, validationInput):\n",
    "    trainData = open(trainInput).read()\n",
    "    validationData = open(validationInput).read() \n",
    "    \n",
    "    return trainData, validationData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Train the linear classifier using your training set. How many mistakes are made before the\n",
    "algorithm terminates? Test your implementation of perceptron test by running it with\n",
    "the learned parameters and the training data, making sure that the training error is zero.\n",
    "Next, classify the emails in your validation set. What is the validation error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the error on the training set:  0.0\n",
      "Number of mistakes made by trainPerceptron: 447\n",
      "This is the error on the validation set:  0.02\n"
     ]
    }
   ],
   "source": [
    "# Part 4\n",
    "\n",
    "# Retrieving Datasets\n",
    "trainData, validationData = readData(\"spam_train.txt\", \"spam_validation.txt\")\n",
    "\n",
    "# Vectorizing email and creating vocab list\n",
    "vocabFrequencyDict , emailVector , a = emailVectorizer(trainData)\n",
    "wordMap, wordList = makeWordList(vocabFrequencyDict,30)\n",
    "    \n",
    "# Training the perceptron classifier\n",
    "weight, totalError, repeatCount = trainPerceptron(trainData)\n",
    "testError = testPerceptron(weight , trainData) \n",
    "testErrorValidate = testPerceptron(weight , validationData)\n",
    "\n",
    "# Priting the results\n",
    "print(\"Training Set Error Level: \", testError)\n",
    "print(\"Number of mistakes made by trainPerceptron:\", totalError) \n",
    "print(\"Validation Set Error Level: \", testErrorValidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "To better understand how the spam classifier works, we can inspect the parameters to see\n",
    "which words the classifier thinks are the most predictive of spam. Using the vocabulary list together with the parameters learned in the previous question, output the 15 words\n",
    "with the most positive weights. What are they? Which 15 words have the most negative\n",
    "weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spamHamIdentify(weight, limit):\n",
    "    # Mapping weight to vocab\n",
    "    sortVocab = {vocab:weight for (vocab, weight) in zip(wordList, weight)}\n",
    "    \n",
    "    # Sorting dictionary using sorted()\n",
    "    sortVocab=sorted(sortVocab.items(), key=lambda x: x[1])\n",
    "    \n",
    "    spamList=sortVocab[:limit] \n",
    "    hamList=(list(reversed(sortVocab[-limit:]))) \n",
    "    \n",
    "    return (spamList,hamList)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 most positive vocabulary and weight in order =  [('sight', 19), ('our', 17), ('yourself', 16), ('remov', 16), ('guarante', 15), ('market', 15), ('nbsp', 15), ('pleas', 15), ('click', 15), ('these', 15), ('deathtospamdeathtospamdeathtospam', 14), ('ever', 14), ('present', 14), ('your', 14), ('major', 13)]\n",
      "15 most negative vocabulary and weight in order=  [('but', -15), ('wrote', -15), ('prefer', -15), ('and', -14), ('i', -13), ('reserv', -13), ('on', -12), ('still', -12), ('technolog', -12), ('sinc', -11), ('copyright', -11), ('url', -11), ('instead', -11), ('upgrad', -11), ('recipi', -11)]\n"
     ]
    }
   ],
   "source": [
    "# Retrieving Datasets\n",
    "trainData, validationData = readData(\"spam_train.txt\", \"spam_validation.txt\")\n",
    "\n",
    "# Vectorizing email and creating vocab list\n",
    "vocabFrequencyDict , emailVector , a = emailVectorizer(trainData)\n",
    "wordMap, wordList = makeWordList(vocabFrequencyDict,30)\n",
    "\n",
    "# Training the perceptron classifier\n",
    "weight, totalError, repeatCount = trainPerceptron(trainData)\n",
    "\n",
    "# Calling the spam or ham identification function\n",
    "spamList,hamList=spamHamIdentify(weight,15)\n",
    "\n",
    "#Printing Results\n",
    "print(\"15 most positive vocabulary and weight in order = \", hamList)\n",
    "print(\"15 most negative vocabulary and weight in order= \", spamList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "Implement the averaged perceptron algorithm, which is the same as your current implementation but which, rather than returning the final weight vector, returns the average\n",
    "of all weight vectors considered during the algorithm (including examples where no mistake was made). Averaging reduces the variance between the different vectors, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainPerceptronAvg(dataInput):\n",
    "    # Retrieving vectorized emails and the feature list\n",
    "    a , emailVector , spamList = emailVectorizer(dataInput)\n",
    "    featureList = makeFeatureList(wordMap, emailVector)\n",
    "\n",
    "    weight = []\n",
    "    for i in range(len((featureList[0]))):\n",
    "        weight.append(0)\n",
    "    \n",
    "    # numpy array for operational modifications\n",
    "    weightAvg = np.array([0 for i in range(len(featureList [0]))])\n",
    "    \n",
    "    # Epochs: the number of repeated iterations\n",
    "    repeatCount = 0\n",
    "    \n",
    "    # Current iteration's update num\n",
    "    curIter = -1 \n",
    "\n",
    "    while not (curIter == 0): \n",
    "        curIter = 0\n",
    "        count = 0\n",
    "        while count < len(featureList):\n",
    "            # Get the y we use to multiple with dot product\n",
    "            if spamList[count] == 0:\n",
    "                checker = -1 \n",
    "            else:\n",
    "                checker = 1\n",
    "            # Dot product is calculated by multiplying feature and weight\n",
    "            product = np.dot(featureList[count], weight)\n",
    "            if product == 0:\n",
    "                product = 1\n",
    "            # Use checker to identify sign\n",
    "            if product*checker <= 0:\n",
    "                curIter += 1 \n",
    "                weight += checker*(featureList[count])\n",
    "            count += 1\n",
    "            avgRepeats = (repeatCount+1)*count\n",
    "            weightAvg = (weightAvg*(avgRepeats -1) + weight)/ avgRepeats\n",
    "\n",
    "        repeatCount += 1\n",
    "    return weightAvg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "Add an argument to both the perceptron and the averaged perceptron that controls the maximum\n",
    "number of passes over the data. This is an important hyperparameter because for large training\n",
    "sets, the perceptron algorithm can take many iterations just changing a small subset of the point -- leading to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7\n",
    "\n",
    "def trainPerceptron(dataInput, noPass):\n",
    "    # Global variables for outside function modification\n",
    "    global wordMap \n",
    "    global wordList \n",
    "    \n",
    "    # Retrieving vectorized emails and the feature list\n",
    "    vocabFrequencyDict , emailVector , spamList = emailVectorizer(dataInput)\n",
    "    wordMap, wordList = makeWordList(vocabFrequencyDict, 30)\n",
    "    featureList = makeFeatureList(wordMap , emailVector)\n",
    "\n",
    "    weight = []\n",
    "    for i in range(len((featureList[0]))):\n",
    "        weight.append(0)\n",
    "\n",
    "    # Epochs: the number of repeated iterations\n",
    "    repeatCount = 0 \n",
    "\n",
    "    # Total Error on updates\n",
    "    totalError = 0 \n",
    "\n",
    "    # Current iteration's update num\n",
    "    curIter = -1 \n",
    "    \n",
    "    \n",
    "    while not (curIter == 0) and ((noPass == -1) or repeatCount < noPass):\n",
    "        curIter = 0 \n",
    "        count = 0\n",
    "\n",
    "        while count < len(featureList):\n",
    "            if spamList[count] == 0: \n",
    "                checker = -1\n",
    "            else: \n",
    "                checker = 1\n",
    "            # Dot product is calculated by multiplying feature and weight\n",
    "            product = np.dot(featureList[count], weight) \n",
    "            if product == 0:\n",
    "                product = 1\n",
    "            # Use checker to identify sign\n",
    "            if product*checker <= 0:\n",
    "                curIter += 1 \n",
    "                weight += checker*(featureList[count])\n",
    "            count += 1\n",
    "        # Total Error incremented with current iteration's update number\n",
    "        totalError += curIter\n",
    "        repeatCount += 1\n",
    "        \n",
    "    return weight , totalError , repeatCount\n",
    "\n",
    "def trainPerceptronAvg(dataInput, noPass):\n",
    "\n",
    "    # Global variables for outside function modification\n",
    "    global wordMap \n",
    "    global wordList\n",
    "    \n",
    "    # Retrieving vectorized emails and the feature list\n",
    "    wordMap , emailVector , spamList = emailVectorizer(dataInput)\n",
    "    wordMap, wordList = makeWordList(vocabFrequencyDict, 30)\n",
    "    featureList = makeFeatureList(wordMap , emailVector)\n",
    "    \n",
    "    weight = []\n",
    "    for i in range(len((featureList[0]))):\n",
    "        weight.append(0)\n",
    "    \n",
    "    # numpy array for operational modifications\n",
    "    weightAvg = np.array([0 for i in range(len(featureList [0]))])\n",
    "    \n",
    "    # Epochs: the number of repeated iterations\n",
    "    repeatCount = 0 \n",
    "    \n",
    "    # Current iteration's update num\n",
    "    curIter = -1 \n",
    "\n",
    "    while not (curIter == 0) and ((noPass == -1) or repeatCount < noPass): \n",
    "        curIter = 0\n",
    "        count = 0\n",
    "        while count < len(featureList):\n",
    "            if spamList[count] == 0:\n",
    "                checker =-1 \n",
    "            else:\n",
    "                checker=1\n",
    "                \n",
    "            # Dot product is calculated by multiplying feature and weight\n",
    "            product = np.dot(featureList[count], weight)\n",
    "            if product == 0: \n",
    "                product = 1\n",
    "\n",
    "            # Use checker to identify sign\n",
    "            if product*checker <= 0:\n",
    "                curIter += 1\n",
    "                weight += checker*(featureList[count])\n",
    "            \n",
    "            count += 1\n",
    "            avgRepeats = (repeatCount+1)*count\n",
    "            # Calculating the average weight by repeatedly calulating average from the previous average value\n",
    "            weightAvg = (weightAvg*(avgRepeats -1) + weight) / avgRepeats\n",
    "        repeatCount += 1\n",
    "    return weightAvg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error for the training set convergedto 0.018 after 10 iterations. Hence hyperparamter was set accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8\n",
    "Experiment with various maximum iterations on the two algorithms checking performance on the validation set. Optionally you can try to change X from question 2. Report the best validation error for the two algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 iteration - trainPerceptron: 0.028\n",
      "# 1 iteration - trainPerceptronAvg: 0.02\n",
      "# 2 iteration - trainPerceptron: 0.029\n",
      "# 2 iteration - trainPerceptronAvg: 0.02\n",
      "# 3 iteration - trainPerceptron: 0.047\n",
      "# 3 iteration - trainPerceptronAvg: 0.018\n",
      "# 4 iteration - trainPerceptron: 0.018\n",
      "# 4 iteration - trainPerceptronAvg: 0.019\n",
      "# 5 iteration - trainPerceptron: 0.027\n",
      "# 5 iteration - trainPerceptronAvg: 0.015\n",
      "# 6 iteration - trainPerceptron: 0.024\n",
      "# 6 iteration - trainPerceptronAvg: 0.02\n",
      "# 7 iteration - trainPerceptron: 0.02\n",
      "# 7 iteration - trainPerceptronAvg: 0.021\n",
      "# 8 iteration - trainPerceptron: 0.025\n",
      "# 8 iteration - trainPerceptronAvg: 0.02\n",
      "# 9 iteration - trainPerceptron: 0.02\n",
      "# 9 iteration - trainPerceptronAvg: 0.019\n",
      "# 10 iteration - trainPerceptron: 0.02\n",
      "# 10 iteration - trainPerceptronAvg: 0.018\n"
     ]
    }
   ],
   "source": [
    "# Part 8\n",
    "\n",
    "trainData, validationData = readData(\"spam_train.txt\", \"spam_validation.txt\")\n",
    "\n",
    "vocabFrequencyDict , emailVector , a = emailVectorizer(trainData)\n",
    "wordMap, wordList = makeWordList(vocabFrequencyDict,30)\n",
    "\n",
    "# based on testing, the error value converges to 0.018 after 10 iterations\n",
    "for i in range(10):\n",
    "    weight, totalError, repeatCount = trainPerceptron(trainData , i+1) # max 11\n",
    "    testErrorValidate = testPerceptron(weight , validationData)\n",
    "    weightAvg = trainPerceptronAvg(trainData , i+1)\n",
    "    testErrorValidateAverage = testPerceptron(weightAvg , validationData)\n",
    "\n",
    "    print(\"#\",i+1,\"iteration - trainPerceptron:\", testErrorValidate)\n",
    "    print(\"#\",i+1,\"iteration - trainPerceptronAvg:\",testErrorValidateAverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "Combine the training set and the validation set (i.e. us all of spam_train.txt) and learn using the best of the configurations previously found. You do not need to rebuild the vocabulary when re-training on the train + validate set. What is the error on the test set (i.e. spam_test.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on Test Set: 0.042\n"
     ]
    }
   ],
   "source": [
    "# Part 9\n",
    "\n",
    "trainData, validationData = readData(\"spam_train.txt\", \"spam_validation.txt\")\n",
    "testData = open(\"Data/spam_test.txt\").read()\n",
    "\n",
    "vocabFrequencyDict , emailVector , a = emailVectorizer(trainData)\n",
    "wordMap, wordList = makeWordList(vocabFrequencyDict,30)\n",
    "\n",
    "#Final Testing with test dataset\n",
    "finalWeight = trainPerceptronAvg(validationData, 5) \n",
    "finalError = testPerceptron(finalWeight , testData)\n",
    "print(\"Error on Test Set:\", finalError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
